{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2465dbf-f204-4eaf-bdad-18b3d7cf359e",
   "metadata": {},
   "source": [
    "## First Load in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0500a9f6-a8c7-4ac5-8777-3e23d752eade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"description\":\"\\u2022\\tIdentify and fix over 400 disruptive transportation shipments per day to increase trucking customer\\u2019s revenue and Shiplify's efficiency\\u2022\\tHelped increase from a 93% success rate to 96% success rate over 3 months for the machine learning software which calculates over 300,000 shipments per day\"}\n",
      "{\"description\":\"Digital Self-Service Launch: Successfully launched over 90% Straight-Through Processing (STP) menus for self-service, integrating Chat-bot technology across Thailand, Indonesia and Singapore.Operational Efficiency: Achieved a 15% reduction in agent inquiries through increased deflection rate, contributing to improved operational efficiency.Strategic Partnerships: Drove strategic partnerships with Fintechs like Avatech.AI,  Walkme, Bancassurance fintechs.\"}\n",
      "{\"description\":\"- Prototyped AI Chatbot and Agents ingested with vector-embedded knowledge base and production feature store using Anthropic Claude and AWS Bedrock.- Built Urgently\\u2019s award winning pricing engine (https:\\/\\/tinyurl.com\\/42n7ared), reducing human intervention by 22% and achieving $1.19MM in annual savings.- Architected and deployed the company's first ML system for provider-customer matching, processing 50k requests\\/day, doubling the match rate, and delivering $2.2MM in annual savings.- Recruited and led a team of data scientists in transforming existing products related to search, pricing, forecasting, and measurement; reported directly to Chief Data Officer.\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "jobs = pd.read_csv(\"Descriptions_and_roles_AI_flagged.csv\")\n",
    "jobs_df = pd.DataFrame(jobs)\n",
    "jobs_df_des = jobs_df[['description']].iloc[:3]\n",
    "# Convert to JSONL (newline-delimited JSON objects)\n",
    "batch_jsonl = jobs_df_des.to_json(orient=\"records\", lines=True)\n",
    "print(batch_jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e2c576e7-a017-4902-a1a0-a70fb73e1766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import re\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "descriptions_token  = os.getenv('description_API')\n",
    "AWS_API = os.getenv('AWS_BEARER_TOKEN_BEDROCK')\n",
    "\n",
    "AI_roles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "44fc520f-205b-4102-b789-c5354e640527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts modified from Gmyrek et al, and Hosseini and Lichtinger \n",
    "\n",
    "root_prompt = \"\"\"Act as a precise classifier for LinkedIn job descriptions. Follow instructions exactly and do not add any commentary or reasoning.\"\"\"\n",
    "\n",
    "core_prompt = f\"\"\"\n",
    "You will be given job descriptions in JSON format. \n",
    "We distinguish three categories: \n",
    "A) LLM INTEGRATOR = descriptions of positions that build/operate LLM-powered systems or embed LLMs into workflows. \n",
    "Signals:RAG(retrieval-augmentedgeneration),embeddings/vectorDB(FAISS/Milvus/Pinecone), prompt engineering at system level, \n",
    "orchestration/agents/LLMOps,LangChain/LlamaIndex,fine-tuning/adapters, model serving/inference, \n",
    "evaluation/guardrails/red-teaming, API integration of LLMs into products or internal processes. \n",
    "B) LLM USER = descriptions of positions that primarily use LLM tools (ChatGPT, Gemini, Copilot, etc.) to perform tasks \n",
    "such as drafting, summarizing, coding assistance, customer responses —without building systems. \n",
    "C) SIGNALING = descriptions of positions that vaguely mention LLMs or AI terminology without actually using or integrating it.\n",
    "\n",
    "NOT in-scope for integrator unless integration is explicit: \n",
    "– Foundation-model pretraining/research scientist roles at model labs (OpenAI/DeepMind/etc.). \n",
    "– Generic ML/NLP with no explicit LLM signals. \n",
    "– Pure labeling/annotation. \n",
    "\n",
    "Edge rules: \n",
    "– If both integration and user aspects appear, set role type= \"both”. \n",
    "– If acronyms like “RAG” or \"AI\" appear, assume the LLM meaning unless context contradicts. \n",
    "\n",
    "Output a string of AI Roles for each description, with a comma delimited separator \n",
    "where AI Role → \"LLM INTEGRATOR\", \"LLM USER\", \"BOTH\", or \"SIGNALING\". Only use the descriptions to make the classification. \n",
    "\n",
    "Do not add extra keys or commentary outside the output.  \n",
    "\n",
    "Here are the job descriptions:\n",
    "\n",
    "{batch_jsonl}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7fdb7802-c91d-497f-8ba6-ebd2f068e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the API call to Hugging Face model. Define a function for this\n",
    "def description_AIrole_labeling(root_prompt, core_prompt, temp):\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://router.huggingface.co/v1\",\n",
    "        api_key=descriptions_token,\n",
    "    )\n",
    "    # Put prompt into a dictionary object. \n",
    "    messages = [{\"role\": \"system\", \"content\": root_prompt},\n",
    "                {\"role\": \"user\", \"content\": core_prompt}]\n",
    "    \n",
    "    # Query the API\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"mistralai/Mistral-7B-Instruct-v0.2:featherless-ai\",\n",
    "            messages=messages,\n",
    "            temperature=temp)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "58088bf2-47eb-4ae0-aab8-c93b68cfb838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the API call to AWS GPT model.\n",
    "def description_AIrole_labeling_AWS(root_prompt, core_prompt):\n",
    "    \n",
    "    # Initialize the Bedrock Runtime client\n",
    "    client = boto3.client('bedrock-runtime')\n",
    "    \n",
    "    # Model ID\n",
    "    model_id = 'openai.gpt-oss-120b-1:0'\n",
    "    \n",
    "    # Create the request body\n",
    "    request = {\n",
    "      \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": root_prompt},\n",
    "        { \"role\": \"user\",\"content\": core_prompt}\n",
    "      ],\n",
    "      \"max_completion_tokens\": 2132,\n",
    "      \"temperature\": 0,\n",
    "      \"top_p\": 0.5,\n",
    "      \"reasoning_effort\": \"low\",\n",
    "    }\n",
    "    \n",
    "    # Make the InvokeModel request\n",
    "    response = client.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps(request)\n",
    "    )\n",
    "    \n",
    "    # Parse and print the message for each choice in the chat completion\n",
    "    response_body = json.loads(response['body'].read().decode('utf-8'))\n",
    "    response = response_body['choices'][0]['message']['content']\n",
    "    response = re.sub(r\"<reasoning>.*?</reasoning>\", \"\", response, flags=re.DOTALL).strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1c0b3bc6-2178-4254-a342-3c94c4509359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIGNALING,LLM INTEGRATOR,LLM INTEGRATOR\n"
     ]
    }
   ],
   "source": [
    "# Print the response\n",
    "result = description_AIrole_labeling_AWS(root_prompt, core_prompt)\n",
    "print(result)\n",
    "roles = result.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "73847488-0242-432d-b9cb-a873d36a7e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['SIGNALING', 'LLM INTEGRATOR', 'LLM INTEGRATOR']]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_roles = []\n",
    "AI_roles.append(roles)\n",
    "AI_roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f22d61fc-21d2-471e-b7e0-01344b3c74a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SIGNALING', 'LLM INTEGRATOR', 'LLM INTEGRATOR']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "list(chain.from_iterable(AI_roles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1808dee-65a4-4bd2-a02a-825234189499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
